{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Na√Øve Bayes Classifier Algorithm\n",
    "2. K Means Clustering Algorithm\n",
    "3. Support Vector Machine Algorithm\n",
    "4. Apriori Algorithm\n",
    "5. Linear Regression\n",
    "6. Logistic Regression\n",
    "7. Artificial Neural Networks & Deep Learning\n",
    "8. Random Forests\n",
    "9. Decision Trees\n",
    "10. k-Nearest Neighbours (kKN)\n",
    "11. Expectation Maximization (EM)\n",
    "12. Page Rank\n",
    "13. AdaBoost\n",
    "14. Classification and Regression Tree (CART)\n",
    "\n",
    "Refer to https://www.dezyre.com/article/top-10-machine-learning-algorithms/202 for a good  overview of most of these"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>AdaBoost</H2>\n",
    "<h4>The concept</h4>\n",
    "Create a good classifier by combining the outputs of multiple weak classifier - a \"wisdom of the crowds\" approach.\n",
    "\n",
    "$$F_T(x) = \\sum_{t=1}^T f_t(x)\\,\\!$$\n",
    "\n",
    "- where T = total number of classifiers\n",
    "- $f_t$ is individual weak classifier, $F_T$ is the strong classifier\n",
    "- x is the input value you wish to classify\n",
    "\n",
    "<h4>The method</h4>\n",
    "<br>\n",
    "1. Given input x and output y, where the y is either a 1 or a -1, and N rows of data\n",
    "2. Initialize your weights, D as 1 / N\n",
    "3. T = number of classifiers you want to try (e.g. T=400)\n",
    "4. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<H2>Naieve Bayes</H2>\n",
    "<br>\n",
    "Naive Bayes is a simple technique for classifying things. It is not a single algorithm but a family of algorithms based on a common principle: all naive Bayes classifiers assume that the value of a particular feature is independent of the value of any other feature, given the class that the data belongs to.<br> \n",
    "For example, a fruit may be considered to be an apple if it is 'red', 'round', and about '10 cm' in diameter. A naive Bayes classifier considers each of these features to contribute independently to the probability that this fruit is an apple, regardless of any possible correlations between the color, roundness, and diameter features.<br>\n",
    "An advantage of naive Bayes is that it only requires a small number of training data to estimate the parameters necessary for classification<br>\n",
    "<br>\n",
    "<b>How it works in a nutshell</b><br>\n",
    "Given the inputs x1,x2,x3 etc. the probability that this belongs to class y1, can be determined by saying assuming y1 is true, then what's the probability of x1,x2,x3 etc. It also assumes that x1,x2,x3 etc. are indepdent features that do not impact the probability of one another <b>given</b> that class.\n",
    "<br>The ability to assume independence under a given class is a crucial part of applied Bayes. So 'great' and tremendous' are postively correlated in a movie review, but if you took it for granted that the movie review was positive, then the probability of 'great' is unlikely to correlate as strongly with 'tremendous' - the two may be equally likely for instance.\n",
    "<br>\n",
    "In text classification:<br>\n",
    "p(document is classified as y, given input labels x) = p(class y) * (p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
