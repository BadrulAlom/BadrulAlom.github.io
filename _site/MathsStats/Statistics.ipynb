{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Mathematical Notation</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What is a probability mass function?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A P.M.F is also known as the probability function, the probability distribution, the frequency function, or probability density function. \n",
    "\n",
    "It basically means the probability of something being X is a number that is between 0 and 1, and if you sum up all the possibilities, the total would come to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b> What is a p-value?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The p-values is the probability of seeing a test result in a probabilty distribution you give it. It is not the probability of H0 being true. I.e. Probability of test data | hypothesis does not equal prob of hypothesis given the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Explain Hypothesis testing<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis testing is about determining whether some value occured by chance. You determine the probability distribution it would follow if something was random and by chance. This is the null hypothesis.\n",
    "\"A general statement or default position that there is no relationship between two measured phenomena, or no association among groups.\"\n",
    "\n",
    "You then measure the probability of seeing the values you are seeing under this distribution (the p-value). If the p-value is quite low, then you infer that there's a high chance it wasn't due to chance, that there was some underlying relationship/reason for it.\n",
    "\n",
    "If the p-value is high, you conclude that it does not disprove the null hypothesis (but it doesn't mean it proves it either)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What is a Type I and Type II error</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type I: Rejecting null hypothesis when it is true - The probability of our pv(X) being less than 0.05,\n",
    "<br>Type II error:  Failing to reject the null hypothesis when it is false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What is the T-Test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The t-test assesses whether the means of two groups are statistically different from each other. This analysis is appropriate whenever you want to compare the means of two groups, and especially appropriate as the analysis for the posttest-only two-group randomized experimental design.\n",
    "\n",
    "\n",
    "A one-sample location test of whether the mean of a population has a value specified in a null hypothesis.\n",
    "A two-sample location test of the null hypothesis such that the means of two populations are equal. All such tests are usually called Student's t-tests, though strictly speaking that name should only be used if the variances of the two populations are also assumed to be equal; the form of the test used when this assumption is dropped is sometimes called Welch's t-test. These tests are often referred to as \"unpaired\" or \"independent samples\" t-tests, as they are typically applied when the statistical units underlying the two samples being compared are non-overlapping.[8]\n",
    "A test of the null hypothesis that the difference between two responses measured on the same statistical unit has a mean value of zero. For example, suppose we measure the size of a cancer patient's tumor before and after a treatment. If the treatment is effective, we expect the tumor size for many of the patients to be smaller following the treatment. This is often referred to as the \"paired\" or \"repeated measures\" t-test:[8][9] see paired difference test.\n",
    "A test of whether the slope of a regression line differs significantly from 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>What is the Wald test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where as the t-test is motivated by small, Gaussian samples, the Wald test uses the same statistic but assumes the sample size is large enough so that the central limit theorem kicks in.\n",
    "–Samples $X_{(i)}$can be of “any” distribution.\n",
    "–Hence the distribution of the statistic (let’s call it W, but it’s the same formula as T) is N(0, 1) now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>What is bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method used when you have a good sample of data but the underlying distribution for it is complex so you cannot use standard probability distributions to estimate.\n",
    "\n",
    "Bootstrapping works by ty treating your sample data as the true population data and taking multiple samples from it. This allows one to gain a sense of the variance of the paramaters of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What are confidence intervals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>What is a Maximum Liklihood Estimate (MLE)?<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the problem with R^2 as a test statistics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R^2$ looks at total sum of squares error -- does not consider the variability of that error around the predicted. E.g. If you have a chart where really there's two distinct regions of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>What does the distribution of P-Values look like?</b>\n",
    "P values have a uniform distribution if the null hypothesis is true. That is to say if the thing you are testing is truly random and there is no correlation then each p-value between 0 and 1 should have an equal chance of getting triggered.\n",
    "\n",
    "http://varianceexplained.org/statistics/interpreting-pvalue-histogram/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-8cf8463b34ca>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-8cf8463b34ca>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    .\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
