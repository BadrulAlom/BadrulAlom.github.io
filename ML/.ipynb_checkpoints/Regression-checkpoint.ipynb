{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is multi-collinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When Independent variables are highly correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What is a Polynomial?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A polynomial is an expression consisting of variables and coefficients which only employs the operations of addition, subtraction, multiplication, and non-negative integer exponents. An example of a polynomial of a single variable x is $x^2 − 4x + 7$. An example in three variables is $x^3 + 2xyz^2 − yz + 1.$\n",
    "\n",
    "The 'degree' of a polynomial is the highest exponent (^) it utilizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomials allow a linear function to add on additional layers of complexity as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/polynomials.jpg\" height=\"200\" width=\"400\">, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e.g. Qunitic function = y(x) = $\\alpha_0+\\alpha_1x+\\alpha_2x+\\alpha_3x+\\alpha_4x$\n",
    "\n",
    "Higher degree poynomials are useful as they can provide a better fit to data if a simple linear model does not suffice. Each extra degree allows an additional level of curvature - notice for quadratic the curve changes once, and for quintic it changes 4 times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What is Polynomial Least Square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Watch this video: https://www.youtube.com/watch?v=Z5iq95Vg2ZY&index=4&list=PLpT5xJ7AmkRW5Q0HwfVRWgUjFdiis-alc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>What is Ridge Regression?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression is a technique that deals with over-fitting due to having too many parameters. If you think of this as resulting in lots of co-efficients that are highly tuned to the dataset it was trained on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What is Robust Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very common to model the noise (i.e. error term) in regression models using a Gaussian distribution with zero mean and constant variance, \u0005i ∼ N (0, σ 2 ), where \u0005i = yi −wT xi . In this case, maximizing likelihood is equivalent to minimizing the sum of squared residuals, as we have seen. However, if we have outliers in our data, this can result in a poor fit. \n",
    "\n",
    "This is because squared error penalizes deviations quadratically, so points far from the line have more affect on the fit than points near to the line. One way to achieve robustness to outliers is to replace the Gaussian distribution for the response variable with a distribution that has heavy tails. Such a distribution will assign higher likelihood to outliers, without having to perturb the straight line to “explain” them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What is the logistic function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x) =\\frac{1}{1+e^{-\\theta}}$$\n",
    "\n",
    "This is the sigmoid function. It has a tigh S shaped curve which is better for approximating to binary (0 or 1) type outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>..and how is this used in logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and also remember that for normal linear regression the output y  = $ h_\\theta (X)$\n",
    "\n",
    "(this last term can be thought of as 'some function of X with parameter theta'  -- remember function can mean a long polynomial formula with $X^n$ and a $\\theta$ vector)\n",
    "\n",
    "In linear regression $ h_\\theta (X)$ is defined as  $\\theta^TX$  (where T = transpose)\n",
    "\n",
    "In logistic regression $ h_\\theta (X)$ is defined as  $\\theta^TX$ wrapped up in a sigmoid function\n",
    "\n",
    "\n",
    "$$= \\frac{1}{1+e^{-(\\theta^Tx_0)}}$$ This is also = prob(y|x and $\\theta$) (e.g. 0.7 = 70% chance of 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In logistic regressioin we say y = 1 when probability >=0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"LogisticFunction.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So given the output $h_\\theta (X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Logistic Regression Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the formula above for the logistic  function we can derive the following loss function to denote the error in the model:\n",
    "\n",
    "$$J(\\theta) = -\\frac1 m \\sum_{i=1}^m\\Big[ y^i log(h_ \\theta x^i)  + (1 - y^i)log (1-h_\\theta (x^i)\\Big]$$\n",
    "This is what we would like to minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Gradient Descent and Newton-Raphson algorithm <b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For logistic regression, the $\\theta$ vector cannot be calculated easily. It requires using gradient descent to find the minmum point of loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Mathematically this means finding a $\\theta$ vector where the derviative of the loss (the change in loss given the change in $\\theta$) approximates to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is done in iterative steps, and the Newton-Raphson method does the iteration efficiently by calculating a second derivative on top of the first derivative:\n",
    "\n",
    "New $\\theta$ = old $\\theta-\\frac {DiffrentialOfLoss} {DiffrentialOfDifferentialOfLoss}$ \n",
    "\n",
    "= $\\theta_{t+1} = \\theta_{t}  - \\frac {f(J(\\theta_{t}))} {f'(J(\\theta_{t}))}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version of the formula works for when your $\\theta$ is not a vector.\n",
    "The generalized version looks like this:\n",
    "\n",
    "= New $\\theta$ = old $\\theta-\\frac {DiffrentialOfLoss} {DiffrentialOfDifferentialOfLoss}$ \n",
    "\n",
    "= New $\\theta$ = old $\\theta$ - DiffrentialOfLoss * inverse of DiffrentialOfDifferentialOfLoss\n",
    "\n",
    "= $\\theta_{t+1} = \\theta_t  - $ GradientVector *  Hessian($\\theta)^{-1}$\n",
    "\n",
    "= $\\theta_{t+1} = \\theta_t  - \\nabla f'(J(\\theta_t))   * H(\\theta)^{-1} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
