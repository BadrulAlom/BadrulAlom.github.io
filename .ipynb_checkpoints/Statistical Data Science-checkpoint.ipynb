{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Mathematical Notation</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Explain Hypothesis testing<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis testing is about determining whether some value occured by chance. You determine the probability distribution it would follow if something was random and by chance. This is the null hypothesis.\n",
    "\"A general statement or default position that there is no relationship between two measured phenomena, or no association among groups.\"\n",
    "\n",
    "You then measure the probability of seeing the values you are seeing under this distribution (the p-value). If the p-value is quite low, then you infer that there's a high chance it wasn't due to chance, that there was some underlying relationship/reason for it.\n",
    "\n",
    "If the p-value is high, you conclude that it does not disprove the null hypothesis (but it doesn't mean it proves it either)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Describe the Bernoulli, Binomal distributions<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benoulli distribution is when the outcome of an event is binary - success or not.\n",
    "\n",
    "Binomial is like a Benoulli but repeated multiple times (e.g. flipping a coin 10 times). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(A) = \\sum P(\\{ (e_1,\\dotsc,e_N) \\})  =  \\frac{N!}{k!(N-k)!} \\cdot p^kq^{N-k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>What is the Wald test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where as the t-test is motivated by small, Gaussian samples, the Wald test uses the same statistic but assumes the sample size is large enough so that the central limit theorem kicks in.\n",
    "–Samples X(i)can be of “any” distribution.\n",
    "–Hence the distribution of the statistic (let’s call it W, but it’s the same formula as T) is N(0, 1) now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What is the T-Test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The t-test assesses whether the means of two groups are statistically different from each other. This analysis is appropriate whenever you want to compare the means of two groups, and especially appropriate as the analysis for the posttest-only two-group randomized experimental design.\n",
    "\n",
    "\n",
    "A one-sample location test of whether the mean of a population has a value specified in a null hypothesis.\n",
    "A two-sample location test of the null hypothesis such that the means of two populations are equal. All such tests are usually called Student's t-tests, though strictly speaking that name should only be used if the variances of the two populations are also assumed to be equal; the form of the test used when this assumption is dropped is sometimes called Welch's t-test. These tests are often referred to as \"unpaired\" or \"independent samples\" t-tests, as they are typically applied when the statistical units underlying the two samples being compared are non-overlapping.[8]\n",
    "A test of the null hypothesis that the difference between two responses measured on the same statistical unit has a mean value of zero. For example, suppose we measure the size of a cancer patient's tumor before and after a treatment. If the treatment is effective, we expect the tumor size for many of the patients to be smaller following the treatment. This is often referred to as the \"paired\" or \"repeated measures\" t-test:[8][9] see paired difference test.\n",
    "A test of whether the slope of a regression line differs significantly from 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>What is bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method used when you have a good sample of data but the distribution for it is complex so you cannot use standard probability distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What are confidence intervals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>What is a Maximum Liklihood Estimate (MLE)?<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the problem with R^2 as a test statistics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R^2 looks at total sum of squares error -- does not consider the variability of that error around the predicted. E.g. If you have a chart where really there's two distinct regions of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
